{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from joblib import dump, load\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer \n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "\n",
    "\n",
    "# Download nltk ressources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "def load_data():\n",
    "    engine = create_engine('sqlite:///data.db')\n",
    "    df = pd.read_sql('select * from emergency_messages', engine)\n",
    "    X = df['message']\n",
    "    Y = df.drop(['message', 'genre'], axis = 1)\n",
    "    return X, Y\n",
    "\n",
    "# Load the data\n",
    "X, Y = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contractions dictionary to replace in the text.\n",
    "contract_dict = {\"ain't\": \"am not\", \"aren't\": \"am not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",  \"he'd've\": \"he would have\", \"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\", \"i'd\": \"I would\", \"i'd've\": \"I would have\", \"i'll\": \"I will\", \"i'll've\": \"I will have\",\n",
    "    \"i'm\": \"i am\", \"i've\": \"I have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"t will\", \"it'll've\": \"it will have\", \"it's\": \"t is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\", \"might've\": \"might have\", \"mightn't\": \"might not\", \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
    "    \"so's\": \"so is\", \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\", \"why've\": \"why have\",\"will've\": \"will have\", \"won't\": \"will not\",\"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def tokenize(text):    \n",
    "    # Finding and replacing urls\n",
    "    text = re.sub('(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', \n",
    "                  'placeholder', text)\n",
    "\n",
    "    # Replace contractions\n",
    "    for key in contract_dict: \n",
    "        text = text.replace(key.lower(), contract_dict[key])\n",
    "    \n",
    "    # Remove special characters\n",
    "    text = re.sub('[^0-9a-zA-Z]+', ' ', text)\n",
    "    \n",
    "    # Lower\n",
    "    text = text.lower()\n",
    "    \n",
    "    # stop words\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    \n",
    "    # Tokens\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Part of speech\n",
    "    pos_tokens = pos_tag(tokens)\n",
    "    \n",
    "    # Lemmatize\n",
    "    clean_tokens = [WordNetLemmatizer().lemmatize(token[0], wordnet_pos(token[1])) \n",
    "                    for token in pos_tokens if token[0] not in stop_words]\n",
    "    \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to select the column with the messages only\n",
    "class SelectText(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X['message'].values\n",
    "\n",
    "# Class to categorical\n",
    "class CategoricalData(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # pre-defining dataframe to be able to predict for one type\n",
    "        type_cat = pd.api.types.CategoricalDtype(categories=['direct', 'news', 'social'], ordered = False)\n",
    "        cat = pd.Series(X['genre'].values, dtype = type_cat)\n",
    "        return pd.get_dummies(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline_cat():\n",
    "    '''\n",
    "    Pipeline if the type of text is available.\n",
    "    '''\n",
    "    pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "\n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('text', SelectText()),\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ])),\n",
    "\n",
    "            ('categorical_data', CategoricalData())\n",
    "        ])),\n",
    "\n",
    "        ('model', MultiOutputClassifier(RandomForestClassifier()))\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "def create_pipeline():\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('model', MultiOutputClassifier(RandomForestClassifier()))\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "pipeline = create_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "           n_jobs=1))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.94\n",
      "Overall f-score: 0.6\n",
      "Overall recall: 0.58\n",
      "Overall precision: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "def show_performance(y_true, y_pred):\n",
    "    # classification report\n",
    "    fscores = []\n",
    "    recalls = []\n",
    "    precisions = []\n",
    "    accuracies = []\n",
    "    \n",
    "    # Transforming preds to dataframe\n",
    "    \n",
    "    y_pred = pd.DataFrame(y_pred, columns = Y.columns)\n",
    "    # Report\n",
    "    for column in y_pred.columns:\n",
    "        report = classification_report(y_test[column],y_pred[column])\n",
    "#         print(report)\n",
    "        precision,recall,fscore,_=score(y_test[column],y_pred[column], average = 'macro')\n",
    "        accuracies.append(accuracy_score(y_test[column], y_pred[column]))\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        fscores.append(fscore)\n",
    "\n",
    "\n",
    "    # Printing metrics\n",
    "    print(f'Overall Accuracy: {round(np.mean(accuracies),2)}')\n",
    "    print(f'Overall f-score: {round(np.mean(fscores),2)}')\n",
    "    print(f'Overall recall: {round(np.mean(recalls),2)}')\n",
    "    print(f'Overall precision: {round(np.mean(precisions),2)}')\n",
    "\n",
    "show_performance(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "[CV] vect__ngram_range=(1, 1) ........................................\n",
      "[CV] ......................... vect__ngram_range=(1, 1), total= 1.5min\n",
      "[CV] vect__ngram_range=(1, 1) ........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ......................... vect__ngram_range=(1, 1), total= 1.5min\n",
      "[CV] vect__ngram_range=(1, 1) ........................................\n",
      "[CV] ......................... vect__ngram_range=(1, 1), total= 1.5min\n",
      "[CV] vect__ngram_range=(1, 2) ........................................\n",
      "[CV] ......................... vect__ngram_range=(1, 2), total= 2.6min\n",
      "[CV] vect__ngram_range=(1, 2) ........................................\n",
      "[CV] ......................... vect__ngram_range=(1, 2), total= 2.6min\n",
      "[CV] vect__ngram_range=(1, 2) ........................................\n",
      "[CV] ......................... vect__ngram_range=(1, 2), total= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed: 16.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "           n_jobs=1))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__ngram_range': [(1, 1), (1, 2)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reducing parameters for speed\n",
    "parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "#     'vect__max_df': [0.75, 1.0],\n",
    "#     'vect__min_df': [0.01, 0.02],\n",
    "#     'tfidf__use_idf': [True, False],\n",
    "#     'model__estimator__n_estimators': [50, 100]\n",
    "#     'model__estimator__min_samples_split': [3, 4],\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters, verbose = 2, n_jobs = -1)\n",
    "\n",
    "# fit\n",
    "cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.94\n",
      "Overall f-score: 0.6\n",
      "Overall recall: 0.58\n",
      "Overall precision: 0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Get the best estimator\n",
    "model = cv.best_estimator_\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Show Performance\n",
    "show_performance(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['related' 'water' 'transport']\n"
     ]
    }
   ],
   "source": [
    "# Tested in the same cell.\n",
    "test = ['thirsty']\n",
    "# predictions\n",
    "predict = model.predict(test)\n",
    "\n",
    "# Get categories\n",
    "print(y_train.columns.values[(predict.flatten()==1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the model as a pickle file\n",
    "def save_model(model, path):\n",
    "    file_model = open(path, 'wb')\n",
    "    pickle.dump(model, file_model)\n",
    "    file_model.close()\n",
    "\n",
    "# Load model \n",
    "def load_model(path):\n",
    "    file_model = open(path, 'rb')\n",
    "    model = pickle.load(file_model)\n",
    "    return model\n",
    "\n",
    "save_model(model, 'best_model_cv.pkl')\n",
    "\n",
    "model = load_model('best_model_cv.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['related' 'water' 'transport']\n"
     ]
    }
   ],
   "source": [
    "test = ['thirsty']\n",
    "# predictions\n",
    "predict = model.predict(test)\n",
    "\n",
    "# Get categories\n",
    "print(y_train.columns.values[(predict.flatten()==1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
